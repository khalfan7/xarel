# Task and seed
task: GauzeRetrieveRL-v0
seed: 42

# Training parameters
total_timesteps: 1_000_000  # Total training steps
learning_starts: 5_000  # Steps before learning starts (random exploration)
log_interval: 10  # Log every N episodes

# Parallel environments
n_envs: 4  # Reduced from 12 to 4 to prevent OOM (each PyBullet env uses ~5-10GB)

# Replay buffer
buffer_size: 1_500_000  
batch_size: 512

# SAC hyperparameters
learning_rate: 3e-4  
tau: 0.02  
gamma: 0.98  
train_freq: 1  
gradient_steps: 1  
ent_coef: 'auto'  
target_update_interval: 1
target_entropy: 'auto'  
use_sde: false  

# Policy network architecture
policy_kwargs:
  net_arch: [512, 512, 512]  

# HER parameters
her:
  strategy: 'future'  # future, episode, random
  n_sampled_goal: 4 
  online_sampling: true 
  handle_timeout_termination: true  

# Environment normalization
normalize: true  
norm_obs: true 
norm_reward: false  
clip_obs: 200.0  
clip_reward: 200.0  

# Evaluation
eval_freq: 10_000  
n_eval_episodes: 20  
save_freq: 20_000  

# Device
device: cuda

hydra:
  run:
    dir: ${hydra:runtime.cwd}/outputs/${task}/seed_${seed}/hydra
