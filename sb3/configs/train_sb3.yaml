# Stable-Baselines3 SAC+HER Training Configuration

# Task and seed
task: GauzeRetrieveRL-v0
seed: 1

# Training parameters
total_timesteps: 1_000_000  # Total training steps
learning_starts: 5_000  # Steps before learning starts (random exploration)
log_interval: 10  # Log every N episodes

# Parallel environments
n_envs: 4  # Reduced from 12 to 4 to prevent OOM (each PyBullet env uses ~5-10GB)

# Replay buffer
buffer_size: 1_500_000  
batch_size: 512

# SAC hyperparameters
learning_rate: 3e-4  # 0.0003
tau: 0.02  # Polyak averaging coefficient
gamma: 0.98  # Discount factor
train_freq: 1  # Update policy every N steps
gradient_steps: 1  # Gradient steps per env step
ent_coef: 'auto'  # Automatic entropy tuning
target_update_interval: 1
target_entropy: 'auto'  # -dim(action_space)
use_sde: false  # State-dependent exploration

# Policy network architecture
policy_kwargs:
  net_arch: [512, 512, 512]  # 3-layer network with 512 units each

# HER parameters
her:
  strategy: 'future'  # future, episode, random
  n_sampled_goal: 4  # Number of virtual transitions per real transition
  online_sampling: true  # Sample HER goals online (more efficient)
  handle_timeout_termination: true  # Handle timeout terminations properly

# Environment normalization
normalize: true  # Normalize observations
norm_obs: true  # Normalize observations
norm_reward: false  # Don't normalize rewards (sparse reward tasks)
clip_obs: 200.0  # Clip normalized observations (common for Fetch tasks)
clip_reward: 200.0  # Clip normalized rewards

# Evaluation
eval_freq: 10_000  # Evaluate every N steps
n_eval_episodes: 20  # Episodes per evaluation
save_freq: 20_000  # Save checkpoint every N steps

# Device
device: cuda

hydra:
  run:
    dir: ${hydra:runtime.cwd}/outputs/${task}/seed_${seed}/hydra
