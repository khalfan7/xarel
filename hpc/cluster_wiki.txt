Logo
Search docs
Policies
Access Requirement for CIAI and CSCC
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 Policies
Cluster Policies and Rules
The following section outlines the rules and policies for utilizing MBZUAI clusters, including CIAI, CSCC, and CAMD.

Please note: Violating any of these restrictions may result in temporary account suspension. Repeated violations can lead to permanent account termination.
Access Requirement for CIAI and CSCC
To gain access to the CIAI or CSCC clusters, users must:

Complete the HPC usage quiz available at: https://forms.gle/ke5BiCRe9nYcQ7Fs8
Achieve a passing score of 14/16.
Email proof of the passing score to: hpc.admins@mbzuai.ac.ae
Access will only be granted after verification by the HPC administration team.
Common Rules
Do not run your job or code directly on the login node.
Do not run 'sleep' jobs via sbatch (jobs which request an allocation but do not use the compute resources).
Use the gpu-debug-qos QOS for all interactive and debugging jobs.
Do not use the salloc command to run jobs; instead, use srun or sbatch.
Do not leave idle bash sessions open when not in use.
Do not use the environment variable CUDA_VISIBLE_DEVICES on CSCC and CIAI cluster to use more GPUs than requested is strictly not allowed and leads to account termination
Do not use the --exclusive option with srun or sbatch.
If your dataset contains a large number of small files on the /l/ filesystem, contact the HPC team for guidance.
The cluster is designed for training tasks. Users are responsible for backing up their own data. Once training is complete, remove unnecessary files.
Upon employee departure, HPC administrators will delete all associated user data. Ensure important data is backed up before leaving.
Access Control
CIAI Cluster
Accessible only to MBZUAI regular faculty (PIs). Each PI is granted one active CIAI account.
Resource access can be delegated by the PI to students or postdocs using the SSH key method. The PI remains fully accountable for any misuse.
General and large-scale compute tasks are permitted through the SLURM queue system, which ensures fair access and efficient workload distribution. Small compute tasks are not allowed in this queue.
Each active CIAI account is limited to a maximum of 3 jobs in the SLURM queue. This limit may vary based on cluster usage; refer to the MBZUAI HPC Wiki for updates.
PIs must contact the cluster management team for CIAI account applications.
CSCC Cluster
Accessible to MBZUAI faculty, researchers, PhD students, Master’s students, and visitors. First-year MSc students require supervisor approval.
Small compute tasks are permitted through the SLURM queue system. General or large-scale compute tasks are not allowed in this queue.
Each CSCC account is limited to a maximum of 2 GPU node jobs in the SLURM queue, with max 4 GPUs allocated.
Contact the cluster management team for CSCC account applications.
CAMD Cluster
Accessible only to MBZUAI regular faculty (PIs). Each PI is granted one active CAMD account.
Resource access can be delegated to students or postdocs using the SSH key method. The PI remains fully accountable for any misuse.
Small, general, and large-scale compute tasks are all permitted through the SLURM queue system, which ensures fair access and efficient workload distribution.
Each active CAMD account is limited to a maximum of 3 jobs in the SLURM queue. This limit may vary based on cluster usage; refer to the MBZUAI HPC Wiki for updates.
PIs must contact the cluster management team for CAMD account applications.
Built with MkDocs using a theme provided by Read the Docs.
Next »
v

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CSCC DOCUMENTATION Connection Guide
Connect to the Cluster
Important Note: Before going through the steps below, you must be connected to the university network through wired, wireless, or VPN connection.

Connection Methods
Windows
You can connect to the cluster using various programs freely available on Windows. Here is a list of the recommended programs:

Visual Studio CodeVSCode will allow you to connect, execute terminal commands, edit and create files, manage Git repos and more. After installing VSCode you can connect to the cluster using the following steps:

Go to the plugin installation menu

1750066011287 2. Search for the Remote SSH plugin

1750066031459 3. Install the Remote SSH plugin

1750066041654 4. Navigate to the new SSH connections menu item on the left bar menu and click on add new

1750066049390 5. Enter the SSH command to connect to the machine

1750066057061 6. Select where to store the SSH config

1750066272291 7. Click on the connect to host button

1750066279541 8. Select Linux as the operating system of the remote machine

1750066287105 9. Enter your university email's password

1750066293350 2. MobaXtermOnce you download and install MobaXterm, you can connect to the cluster through the following steps.

1750066300957 3. PuTTYPuTTY is a lightweight SSH client that will allow you to connect and run commands on the cluster. However, PuTTY will not allow you to move files or edit them using programs on your machine.

Connect using PuTTY

Connect using PuTTY

Connect using PuTTY 4. FileZillaFileZilla allows you to transfer files between your machine and the cluster.

1750066356069 5. WinSCP WinSCP's main function is file transfer between a local and a remote computer. Beyond this, WinSCP offers scripting and basic file manager functionality.

1750066369463

MacOS
CyberDuck
Linux
You can connect directly from the terminal using the following command:

ssh user.name@ciai.mbzuai.ac.ae
password: email_password
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Running Experiments
Examples
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CSCC DOCUMENTATION Slurm Running Experiments
# Running and Managing Experiments

SLURM Commands
sinfo
squeue
srun
sbatch
scancel
Slurm partitions
Slurm partitions are essentially different queues that point to collections of nodes.

Slurm partitions are essentially different queues that point to collections of nodes.

Note: Email notification flags (e.g., --mail-type, --mail-user) are not available on this cluster and will not work for job submission.

CSCC Partitions
The CSCC has two main partitions:

cscc-gpu-p
This partition can be used when the job requires GPU processing. The current restrictions on jobs using this partition are:

Maximum of 2 GPU node jobs, maximum 4 GPU cards allocated per account.
Maximum of 24 hour per job.
Minimum of 1 gpu to request using the --gres=gpu:1
In addition to the partition, you must specify the -q cscc-gpu-qos flag in your sbatch and srun scripts.
cscc-cpu-p
This partition can be used when only CPU processing is required. The current restrictions on jobs using this partition are:

Maximum of 512 CPU cores with a maximum of 2 running jobs
Maximum of 72 hour jobs.
Maximum of 512 CPU cores.
In addition to the partition, you must specify the -q cscc-cpu-qos flag in your sbatch and srun scripts.
If you do not specify a partition when running a job. It will be automatically assigned to the CPU partition.

Slurm QOS
Two QoSes control GPU jobs on cscc-gpu-p:

cscc-gpu-qos (standard / production)
Use for regular batch jobs.
Limits: up to 24h runtime; up to 4 GPUs per user; up to 2 GPU-node jobs per account.
Interactive use: not intended (use debug-gpu-qos for srun/salloc). Example (batch): sbatch -p cscc-gpu-p -q cscc-gpu-qos my_job.sh
debug-gpu-qos (debug / short / interactive)
Use for short tests and interactive sessions.
Limits: up to 3h; up to 4 GPUs per user.
Typical usage: srun / salloc (interactive) or short sbatch tests. Examples:
# interactive
srun -p cscc-gpu-p -q debug-gpu-qos --gres=gpu:1 --cpus-per-task=8 --pty bash

# short batch test
sbatch -p cscc-gpu-p -q debug-gpu-qos test.sh
Choosing: Use cscc-gpu-qos for production batch runs; use debug-gpu-qos for interactive or quick debugging.

Submitting a job with srun
Run a single GPU job
To run a job using a single GPU and 8 CPU cores:

srun --ntasks=1 --cpus-per-task=8 -p cscc-gpu-p -q debug-gpu-qos --gres=gpu:1 --output=./slurm-%N-%j.out python my_python_script.py
Running a CPU job
To run a job using 128 CPU cores:

srun --ntasks=1 --cpus-per-task=128 -p cscc-cpu-p -q cscc-cpu-qos --output=./slurm-%N-%j.out python my_python_script.py
To run a job using 2 CPU cores:

srun --ntasks=1 --cpus-per-task=2 -p cscc-cpu-p -q cscc-cpu-qos --output=./slurm-%N-%j.out python my_python_script.py
Submitting a job with sbatch
sbatch
You can find full examples of running jobs on the examples page.

Monitoring Jobs
You can check the full queue on the cluster by simply executing the squeue command. However, if you just like to view your jobs, you can use:

squeue --me
To keep track of the queue continuously, you can use the watch command along with squeue:

watch -n1 squeue
This will refresh the squeue status every second.

Additional Useful Commands
Check node availability and utilization
Use sinfo to view the status of all nodes: sinfo For a summarized view: sinfo -s

Check estimated start time of your job
To see when your job is expected to start: squeue --start

Cancelling Jobs
To cancel a job, you can use the scancel command as follows:

scancel $JOB_ID
To cancel all jobs from your account, run:

scancel -u $USER
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Running Experiments
Examples
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CSCC DOCUMENTATION Slurm Examples
Examples
Let's start running a simple example to train a classifier on the MNIST dataset. 1- Create the test.py script into your CSCC folder.

import subprocess

subprocess.run(["nvidia-smi"])
2- Activate your conda environment before asking for resources on the cluster!

conda activate your_envirnoment
3- Create the slurm_script indicating the resources to allocate in the cluster and the commands to run.

cat > slurm_script << EOL
#!/bin/bash
#SBATCH --job-name=test             # Job name
#SBATCH --output=output.%A_%a.txt   # Standard output and error log
#SBATCH --nodes=1                   # Run all processes on a single node  
#SBATCH --ntasks=1                  # Run on a single CPU
#SBATCH --mem=40G                   # Total RAM to be used
#SBATCH --cpus-per-task=64          # Number of CPU cores
#SBATCH --gres=gpu:1                # Number of GPUs (per node)
#SBATCH -p cscc-gpu-p               # Use the gpu partition
#SBATCH --time=12:00:00             # Specify the time needed for your experiment
#SBATCH --qos=cscc-gpu-qos          # To enable the use of up to 8 GPUs

hostname
python test.py
EOL
4- Run the model on the CSCC cluster using the sbatch command.

sbatch slurm_script
5- Observe the output in the log file output.txt.

gpu-02
Thu Sep 26 10:56:01 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================+
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   26C    P0             52W /  400W |       4MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   25C    P0             51W /  400W |       4MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   25C    P0             53W /  400W |       4MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   25C    P0             52W /  400W |       4MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
Using Anaconda for the first time
Creating Virtual Environments
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CSCC DOCUMENTATION Software Management
Anaconda and Framework Installation
Using Anaconda for the first time
Anaconda is not activated by default for users. To use Anaconda, you should execute the following command:

source /apps/local/conda_init.sh
If you want Anaconda to be activated every time you login then you should run the following command:

echo "source /apps/local/conda_init.sh" >> ~/.bashrc
Creating Virtual Environments
You can create a virtual environment in your home directory or on the /l storage using the following command:

conda create -p /path/to/env
where /path/to/env is the path to your environment. You can load this environment using:

conda activate /path/to/env
Additionally, you can load the environment by default when you log in by:

echo "conda activate /path/to/env" >> ~/.bashrc
Installing Frameworks
PyTorch
After installing and activating Anaconda using the above steps, you can follow the steps to install PyTorch from >

TensorFlow 2
You can download and install TensorFlow 2 within an Anaconda environment using the steps listed on the [official >

TensorFlow 1.15
The official website for TensorFlow shows that the old version of tensorflow 1.15 only supports Python 3.7 and CU>

conda create -p /path/to/env python=3.8
conda activate /path/to/env
Then, we start to install TensorFlow 1.15 in the conda environment:

pip install --upgrade pip
pip install nvidia-pyindex
pip install nvidia-tensorflow[horovod]
pip install nvidia-tensorboard==1.15
Finally, to test that it works we can execute the following:

import tensorflow as tf
import tensorboard

tf.enable_eager_execution()
a = tf.random.uniform([1000, 1000])
b = tf.random.uniform([1000, 1000])
tf.matmul(a, b)
Common Issues and Solutions
CUDA Version Mismatch
If you encounter CUDA version issues:

# Check CUDA version
nvcc --version

# Make sure your framework uses the correct CUDA version
# For PyTorch, you can specify the CUDA version during installation
Memory Issues During Package Installation
If you encounter memory issues during package installation:

# Use pip with no-cache-dir option
pip install --no-cache-dir package_name

# Or install packages one by one instead of all at once
Package Conflicts
If you encounter package conflicts:

# Create a new conda environment with specific Python version
conda create -n clean_env python=3.8 -y
conda activate clean_env

# Install packages in a specific order, starting with main frameworks
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Node Information
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CAMD DOCUMENTATION Hardware Specs
Cluster Details
The cluster is equipped with 1,000 AMD MI210 GPUs, distributed across approximately 125 worker nodes (8 GPUs per node). Slurm is deployed as the workload manager to handle resource allocation within the cluster.

The cluster consists of multiple components:

Slurm Controller Nodes – Managed exclusively by the Core42 HPC Ops team. These nodes control the cluster and handle Slurm-related configurations, policies, and QoS.

Login Nodes – Act as the landing zone for the cluster. When users connect, they are directed here by default. From the login nodes, users can prepare their environments and submit jobs to Slurm, which will then allocate resources based on the requests specified in their batch scripts.

Worker/GPU Nodes – These are the AMD GPU nodes where users' workloads are executed. They handle training tasks, inference workloads, and model hosting.

Shared Storage – In this cluster, VAST serves as the shared storage system, hosting users' home directories and providing a common storage space accessible across the cluster.

Node Information
Hostname	IP Address	Role of the Hosts
auh-1b-cpu-login-001	172.27.112.247	Login Nodes
auh-1b-cpu-login-002	172.27.112.248	Login Nodes
Important Note: The cluster is hosted in a secured environment where nodes are not allowed to access the internet by default. Any required repo/domain access from the login nodes must be explicitly approved and configured.

In such cases, users should send an email to Core42 AI Cloud Support with the domain(s) that need to be whitelisted in the proxy, along with prior approval from the cluster owner.

GPU Performance Comparison
All benchmarks use bf16 precision. For 4-GPU DDP, global batch size is scaled by 4×.

CNN (bf16)
GPU Model	#GPUs	Step Time (s)	Throughput
MI210	1	0.165	1552 images/s
A100-SXM4-40G	1	0.061	4148 images/s
RTX 6000 Ada	1	0.097	2633 images/s
MI210	4	0.167	6105 images/s
A100-SXM4-40G	4	0.071	14399 images/s
RTX 6000 Ada	4	0.115	8903 images/s
GPT (bf16)
GPU Model	#GPUs	Step Time (s)	Throughput
MI210	1	0.249	2050 tokens/s
A100-SXM4-40G	1	0.202	2523 tokens/s
MI210	4	0.299	6842 tokens/s
A100-SXM4-40G	4	0.259	7905 tokens/s
Tips for Consistent Measurements
Fix batch size and sequence length (for GPT), and input resolution (for CNN).
Report the median over multiple steps after warm-up.
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CAMD DOCUMENTATION Connection Guide
How to Obtain Access to Cluster
To obtain access to the cluster, new users should send an email to Core42 AI Cloud Support at <support.aicloud@core42.ai>, including their SSH public key. Users must also secure prior approval from the cluster owner to streamline the process.

Note: As part of the process, the Core42 AI Cloud Support team will request the user to complete a User Request Form, which must be digitally approved by the cluster owner before access is granted.

How to Login to Cluster
There are multiple methods to login to the cluster, but here we're describing the very basic one, however users are allowed to use any other method which they feel more confident/experienced.

Windows
You have several options for logging into a Linux machine from a Windows system, the most common are SSH (for command line).

Using Command-Line (Windows PowerShell) - Windows 10/11 already have an SSH client built in. - Open PowerShell (press Win + X, Windows PowerShell or Terminal). - Run: bash
  ssh username@linux_machine_ip - Example: bash
  ssh core42@172.27.112.247

Linux/MAC
Open a terminal and try the following commands:

Run: bash
  ssh -i <path-to-your-key> username@linux_machine_ip
Example: bash
  ssh -i ~/.ssh/id_rsa.pub core42@172.27.112.247
Note: Access to the cluster is based on SSH key authentication, so it will not prompt for a password if you are using the correct SSH public key.

Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »
Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CAMD DOCUMENTATION Porting Code
Porting Code to AMD ROCm (MI210, ROCm 6.3)
The MI200 series on this cluster targets ROCm 6.3.
PyTorch device APIs: Calls like tensor.cuda(), to(device="cuda") are translated by ROCm/HIP—no code changes needed.
Distributed training: backend="nccl" is automatically mapped to RCCL on ROCm.
Monitoring tools:
Use rocm-smi instead of nvidia-smi.
Use nvtop (supports AMD/ROCm) instead of nvitop.
FlashAttention:
No prebuilt wheels; build from source: FlashAttention GitHub
Alternative: Use PyTorch’s scaled_dot_product_attention for portability.
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
Basic Commands
Job Submission
Job Monitoring
Job Control
Resource Requests (Common sbatch/srun options)
Job Submission
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CAMD DOCUMENTATION Slurm Basic Commands
Slurm Basic Commands
Here's a compact Slurm basics cheat sheet you can use to get started, good for quick reference when submitting and managing jobs.

Job Submission
Command	Purpose	Example
sbatch script.sh	Submit a batch job	sbatch myjob.sh
srun command	Run a command (interactive or parallel)	srun hostname
salloc	Allocate resources and get an interactive shell	salloc -N 2 -n 4
Job Monitoring
Command	Purpose	Example
squeue	List jobs in the queue	squeue
squeue -u username	Show only your jobs	squeue -u core42
sacct	Show job accounting info (after completion)	sacct -j 12345
Job Control
Command	Purpose	Example
scancel job_id	Cancel a job	scancel 12345
scancel -u username	Cancel all your jobs	scancel -u core42
Resource Requests (Common sbatch/srun options)
Option	Meaning	Example
-N	Number of nodes	-N 2
-n	Total tasks (MPI ranks)	-n 8
--cpus-per-task	CPU cores per task	--cpus-per-task=4
--gres=gpu:count	Request GPUs	--gres=gpu:4
-t	Time limit (HH:MM:SS)	-t 01:30:00
-p	Partition/queue	-p gpu
-J	Job name	-J myjob
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
Basic Commands
Job Submission
Submit a job script:
Interactive shell on a GPU node:
Run MPI program on 4 nodes:
Example Slurm Batch Script
Explanation
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CAMD DOCUMENTATION Slurm Job Submission
How to Submit Jobs
Submit a job script:
sbatch -N 1 -n 4 -t 00:30:00 job.sh
Interactive shell on a GPU node:
salloc -N 1 --gres=gpu:1 -t 02:00:00
Run MPI program on 4 nodes:
srun -N 4 -n 64 ./my_mpi_app
Example Slurm Batch Script
Here's a basic Slurm batch script that runs on 8 nodes, each with 8 AMD GPUs (like your MI210 setup), for a total of 64 GPUs:

#!/bin/bash
#SBATCH -J my_gpu_job              # Job name
#SBATCH -N 8                       # Number of nodes
#SBATCH --ntasks-per-node=8        # 1 task per GPU
#SBATCH --gres=gpu:8               # GPUs per node
#SBATCH -p gpu                     # Partition/queue name
#SBATCH -t 02:00:00                # Time limit hh:mm:ss
#SBATCH -o job_%j.out              # Standard output
#SBATCH -e job_%j.err              # Standard error

# Optional: Set CPU cores per GPU task
#SBATCH --cpus-per-task=8

# Load necessary modules
module load rocm
module load mpi

# Debug info (optional)
echo "Running on nodes:"
srun hostname | sort -u
echo "SLURM Job ID: $SLURM_JOB_ID"
echo "Tasks: $SLURM_NTASKS"
echo "GPUs per node: 8"
echo "Total GPUs: $(( $SLURM_NNODES * 8 ))"

# Run your application
srun ./my_gpu_program
Explanation
#SBATCH -N 8 → 8 nodes total.
--ntasks-per-node=8 → one MPI rank (or task) per GPU.
--gres=gpu:8 → request 8 GPUs per node.
--cpus-per-task=8 → bind CPU cores for each GPU task (tweak as needed).
module load rocm → load AMD ROCm stack for MI210 GPUs.
module load mpi → load MPI library (e.g., OpenMPI, MPICH).
srun → runs the program across allocated resources.
Further Usage Examples
Request Nodes / GPUs (Slurm)
From the helper tools directory:

The script splits <TOTAL_GPUS> across multiple single-node jobs (max 8 GPUs per job) and submits them.
Example: - Submits two jobs named like myproj-g8-1 and myproj-g4 → one job with 8 GPUs, another with 4 GPUs.

cd /vast/users/guangyi.chen/slurm_tools
./submit_job.sh <NAME> <TOTAL_GPUS>
# Example:
./submit_job.sh myproj 12
Attach to a Running Job
Find <JOBID> using squeue -u $USER, then:

cd /vast/users/guangyi.chen/slurm_tools
./attach_job.sh <JOBID>
# Example:
./attach_job.sh 343
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Login Nodes
GPU Nodes
CPU Nodes
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CIAI DOCUMENTATION Hardware Specs
CIAI Cluster Hardware Specifications
Login Nodes
2 Login Nodes: CIAI-login-1 and CIAI-login-2
CPU: Dual AMD EPYC 7402 (24 cores each)
Total Cores: 48
RAM: 256 GB
GPU Nodes
Total GPU Nodes: 39
CPU: 2 × AMD EPYC 7742 (64 cores each, 128 cores total, 256 threads)
RAM: 256 GB per node
GPUs: 4 × NVIDIA A100 SXM (40 GB each) per node
Local Storage: High-speed NVMe scratch
CPU Nodes
Total CPU Nodes: 5
CPU: 2 × AMD EPYC 7742 (64 cores each, 128 cores total, 256 threads)
RAM: 256 GB per node
Local Storage: Scratch space for temporary files
Storage Locations and Quotas
1. Lustre Storage (/l/users/$USER)
This is where large files like datasets and checkpoints should be stored. The default quota is 2 TB.

To check your current usage and quota on the Lustre filesystem, run:

lfs quota -u $USER /l
2. Home Directory ($HOME)
This is intended for code, logs, and small files. The default quota is 100 GB*.

To view your useage, run:

du -sh $HOME
This shows how much space you're currently using in your home directory.

3. Scratch Space (/tmp)
This is fast, local storage on compute nodes. It’s not shared meant to only serve as local temporary storage.

To view usage, run:

du -sh /tmp/$USER
Note: This only works if your job or session has created a personal directory under /tmp.

Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CIAI DOCUMENTATION Connection Guide
Connect to the Cluster
Important Note: Before going through the steps below, you must be connected to the university network through wired, wireless, or VPN connection.

Connection Methods
Windows
You can connect to the cluster using various programs freely available on Windows. Here is a list of the recommended programs:

Visual Studio CodeVSCode will allow you to connect, execute terminal commands, edit and create files, manage Git repos and more. After installing VSCode you can connect to the cluster using the following steps:

Go to the plugin installation menu

1750066011287 2. Search for the Remote SSH plugin

1750066031459 3. Install the Remote SSH plugin

1750066041654 4. Navigate to the new SSH connections menu item on the left bar menu and click on add new

1750066049390 5. Enter the SSH command to connect to the machine

1750066057061 6. Select where to store the SSH config

1750066272291 7. Click on the connect to host button

1750066279541 8. Select Linux as the operating system of the remote machine

1750066287105 9. Enter your university email's password

1750066293350 2. MobaXtermOnce you download and install MobaXterm, you can connect to the cluster through the following steps.

1750066300957 3. PuTTYPuTTY is a lightweight SSH client that will allow you to connect and run commands on the cluster. However, PuTTY will not allow you to move files or edit them using programs on your machine.

Connect using PuTTY

Connect using PuTTY

Connect using PuTTY 4. FileZillaFileZilla allows you to transfer files between your machine and the cluster.

1750066356069 5. WinSCP WinSCP's main function is file transfer between a local and a remote computer. Beyond this, WinSCP offers scripting and basic file manager functionality.

1750066369463

MacOS
CyberDuck
Linux
You can connect directly from the terminal using the following command:

ssh user.name@ciai.mbzuai.ac.ae
password: email_password
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Running Experiments
Slurm partitions
GPU Job Submission
Running a 3 Nodes, 12 GPUs PyTorch Distributed Data Parallel (DDP) Job
Monitoring Jobs
Cancelling Jobs
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Contact Us
 CIAI DOCUMENTATION Slurm Running Experiments
Running and Managing Experiments
Slurm partitions
Slurm partitions are essentially different queues that point to collections of nodes.

Slurm partitions are essentially different queues that point to collections of nodes.

Note: Email notification flags (e.g., --mail-type, --mail-user) are not available on this cluster and will not work for job submission.

CIAI Partitions
The CIAI has two main partitions:

long
This partition can be used when the job requires GPU processing. The current restrictions on jobs using this partition are:

Maximum of 12 GPUs or 3 jobs.
Maximum of 72 hour per job.
Minimum of 4 gpu to request using the --gres=gpu:4
QOS is gpu-12
cscc-cpu-p
This partition can be used when only CPU processing is required and is shared with CSCC accounts. The current restrictions on jobs using this partition are:

Maximum of 512 CPU cores with a maximum of 2 running jobs
Maximum of 72 hour jobs.
Maximum of 512 CPU cores.
QOS is cscc-cpu-qos
If you do not specify a partition when running a job. It will be automatically assigned to the CPU partition.

GPU Job Submission
Submitting a Basic 1-Node Job
Once you login the cluster, you could submit a job to run your code with GPUs. This is a job script you can copy paste. Let say you save it as in $HOME/job.sh

#!/bin/bash
#SBATCH --time=2:00:00
#SBATCH --nodes=1
#SBATCH -p long
#SBATCH -q gpu-12
#SBATCH --gres=gpu:4
#SBATCH --mem=230G
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=64

# Replace this with your actual job or command
nvidia-smi
After saving this job script, you will submit the job using sbatch command as follow. Each submitted job will be assigned a JobID. In this example, that’s 35189.

sbatch job.sh Submitted batch job 35189 This will schedule your job with maximum 2 hours allowed run time --time=2:00:00 , 1 node --nodes=1. Each node is with 4 40GB A100 GPUs.

This job script run the example command nvidia-smi. You can change it to any bash commmand you want to run. E.g., python whatever-python-code.py

The other parameters are system specific. Including:

--cpus-per-task=64: How many cpu cores will be allocated to your task, per node. If your job is not CPU-heavy, please request less cores (10 is enough for most tasks). 
--gres=gpu:4: Physically there are 4 GPUs per node.
--mem-per-cpu=230G : Physically 230GB system memory allowed per node. Warning: This is not GPU memory.
After the job is finished, the stdout / stderr will be available in a file, under the folder you submitted the job.
Submitting a Multi-Node Job
Submitting a multi-node job is as simple as the following:

#!/bin/bash
#SBATCH --time=2:00:00
#SBATCH --nodes=2
#SBATCH -p long
#SBATCH -q gpu-12
#SBATCH --gres=gpu:4
#SBATCH --mem=230G
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=128

# Replace this with your actual command
srun hostname
srun nvidia-smi
The example output, which will be very long, will be:

gpu-11
gpu-11
gpu-11
gpu-11
gpu-12
gpu-12
gpu-12
gpu-12
Thu Feb  8 15:07:29 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
Thu Feb  8 15:07:29 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
...
The line

#SBATCH --nodes=2
means you requested a job with 2 nodes. That would be 2x4=8 A100 GPUs. Another key difference is

srun hostname
srun nvidia-smi
These 2 lines use srun to spawn the processes across the 2 nodes. 4 each node, 8 total. So you will see the srun hostname generates 8 lines of output, 4 on the first node and 4 on the second. If you examine the output of srun nvidia-smi, you will find it exactly the same.

From here, you can follow your favourite Deep Learning Framework’s multi-node guide to run your job. Like PyTorch DDP (https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html) or Megatron-LM (https://github.com/NVIDIA/Megatron-LM).

Running a 3 Nodes, 12 GPUs PyTorch Distributed Data Parallel (DDP) Job
This is a complete guide to running a 3 nodes, 12 A100 GPUs PyTorch DDP job example. For reference, check (https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#initialize-ddp-with-torch-distributed-run-torchrun)

First, we create a simple PyTorch script that run DDP on a toy model. Save it as elastic_ddp.py.

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim

from torch.nn.parallel import DistributedDataParallel as DDP

import socket

class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(10, 5)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))


def demo_basic():
    dist.init_process_group("nccl")
    rank = dist.get_rank()
    print(f"Start running basic DDP example on rank {rank}.")

    # create model and move it to GPU with id rank
    device_id = rank % torch.cuda.device_count()
    print('I am rank {} using GPU {} on host {}'.format(rank, device_id, socket.gethostname()))
    model = ToyModel().to(device_id)
    ddp_model = DDP(model, device_ids=[device_id])

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(device_id)
    loss_fn(outputs, labels).backward()
    optimizer.step()
    dist.destroy_process_group()

if __name__ == "__main__":
    demo_basic()
The script is pretty straightforward.

Then, create the bash script to launch torchrun. Save the following contents as torchrun_script.sh.

#!/bin/bash

torchrun  --nnodes=$SLURM_JOB_NUM_NODES --nproc_per_node=4 --rdzv_id=1009 --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:29401 elastic_ddp.py
Don’t forget to add execute permission to this script. E.g.,

chmod +x torchrun_script.sh 
After that, create the job script as ddp.sh. Make sure you have PyTorch installed and activated properly. In this example, PyTorch, inside a Conda environment, is activated by the line export PATH=${PWD}/.conda/bin:$PATH

#!/bin/bash
#SBATCH --time=2:00:00
#SBATCH --nodes=3
#SBATCH -p long
#SBATCH -q gpu-12
#SBATCH --gres=gpu:4
#SBATCH --mem=230G
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=128

export PATH=${PWD}/.conda/bin:$PATH

export MASTER_ADDR=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)

srun ./torchrun_script.sh
That is! To launch this job:

sbatch ddp.sh 
In this example, the job id is

Submitted batch job 38867
To examine the output

grep GPU slurm-38867.out  | sort
I am rank 0 using GPU 0 on host gpu-46
I am rank 10 using GPU 2 on host gpu-48
I am rank 11 using GPU 3 on host gpu-48
I am rank 1 using GPU 1 on host gpu-46
I am rank 2 using GPU 2 on host gpu-46
I am rank 3 using GPU 3 on host gpu-46
I am rank 4 using GPU 0 on host gpu-47
I am rank 5 using GPU 1 on host gpu-47
I am rank 6 using GPU 2 on host gpu-47
I am rank 7 using GPU 3 on host gpu-47
I am rank 8 using GPU 0 on host gpu-48
I am rank 9 using GPU 1 on host gpu-48
That is! 12 GPUs on 3 different nodes are utilized by this job.

The next step for you: modify the elastic_ddp.py to the actual model you want to train on. If necessary, pass the model specific arguments to elastic_ddp.py in torchrun_script.sh. If you want, modify ddp.sh for job related parameters like maximum walltime, and etc.

Monitoring Jobs
You can check the full queue on the cluster by simply executing the squeue command. However, if you just like to view your jobs, you can use:

squeue --me
To keep track of the queue continuously, you can use the watch command along with squeue:

watch -n1 squeue
This will refresh the squeue status every second.

Additional Useful Commands
Check node availability and utilization
Use sinfo to view the status of all nodes: sinfo For a summarized view: sinfo -s

Check estimated start time of your job
To see when your job is expected to start: squeue --start

Cancelling Jobs
To cancel a job, you can use the scancel command as follows:

scancel $JOB_ID
To cancel all jobs from your account, run:

scancel -u $USER
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
macOS and Linux
Windows (Using PowerShell)
Windows (Using PuTTYgen)
Frequently Asked Questions
Contact Us
 GENERAL INFO SSH Key Generation
Generating SSH Keys for Cluster Access
Access to the CIAI and CSCC may require authentication using SSH keys. Follow the steps below to generate a public/private key pair on your operating system.

macOS and Linux
Open a terminal.
Run the following command (replace user.name@mbzuai.ac.ae with your email): ssh-keygen -t rsa -b 4096 -C "user.name@mbzuai.ac.ae"
When prompted:
Press Enter to accept the default file location (~/.ssh/id_rsa).
Optionally set a passphrase for extra security.
Your keys will be generated:
Private key: ~/.ssh/id_rsa
Public key: ~/.ssh/id_rsa.pub
Display your public key: cat ~/.ssh/id_rsa.pub
Copy the entire output and send it to the HPC team as instructed.
Windows (Using PowerShell)
Open PowerShell.
Run: ssh-keygen -t rsa -b 4096 -C "user.name@mbzuai.ac.ae""
Accept the default location (C:\Users\<YourUsername>\.ssh\id_rsa) or specify a custom path.
Optionally set a passphrase.
To view your public key: Get-Content $env:USERPROFILE\.ssh\id_rsa.pub
Copy the entire key and send it to the HPC team.
Windows (Using PuTTYgen)
Download and install PuTTYgen.
Open PuTTYgen and click Generate.
Move your mouse around the blank area to generate randomness.
Once generated:
Click Save private key to store your private key.
Copy the text from Public key for pasting into OpenSSH authorized_keys file.
Send the copied public key to the HPC team.
Important Notes
Never share your private key (id_rsa or .ppk file).
Only send the public key (id_rsa.pub or the text from PuTTYgen).
Store your private key securely on your device.
Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »

Logo
Search docs
Policies
CSCC DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
Software Management
CAMD DOCUMENTATION

Hardware Specs
Connection Guide
Porting Code
Slurm
CIAI DOCUMENTATION

Hardware Specs
Connection Guide
Slurm
GENERAL INFO

SSH Key Generation
Frequently Asked Questions
Q: I get the error ssh: connect to host cscc.mbzuai.ac.ae port 22: Operation timed out when trying to connect to the HPC. What should I do?
Q: When I try to copy files using scp, I get the error:
Contact Us
 GENERAL INFO Frequently Asked Questions
Frequently Asked Questions
Q: I get the error ssh: connect to host cscc.mbzuai.ac.ae port 22: Operation timed out when trying to connect to the HPC. What should I do?
A: This usually happens if the cscc hostname is not reachable from your network. Try replacing cscc with ciai in your SSH command:

ssh username@ciai.mbzuai.ac.ae
If the issue persists, check your internet connection or contact HPC support.

Q: When I try to copy files using scp, I get the error:
subsystem request failed on channel 0
scp: Connection closed
A: This happens because the remote server does not allow scp without specifying the correct option. To fix this, include the -O flag in your scp command:

scp -O RL_test.py username@ciai.mbzuai.ac.ae:/users/username
The -O flag forces the use of the legacy SCP protocol, which resolves this issue.

Built with MkDocs using a theme provided by Read the Docs.
« Previous
Next »


---

given the above information. what should a .sh run in order to setup and nevionment ro un an RL script and how do i deploy training in a CIAI cluster

Linux ciai-login-1 5.15.161-ql-generic-13.0-14 #1 SMP Wed Jun 26 16:19:39 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
Ubuntu 22.04.4 LTS

The programs included with the Qlustar/(Debian,Ubuntu) system are mostly free
software; the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Qlustar/(Debian,Ubuntu) comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.

╔════════════════════════════════════════════════════════════════════════════╗
║                                                                            ║
║              Welcome to the CIAI/CSCC High Performance Cluster             ║
║                                                                            ║
║════════════════════════════════════════════════════════════════════════════║
║                                                                            ║
║            ░▒▓█  NEW: CSCC QoS & INTERACTIVE POLICY UPDATE  █▓▒░           ║
║                                                                            ║
║  • QoSes:                                                                  ║
║      - Normal:  cscc-gpu-qos                                               ║
║      - Debug:   gpu-debug-qos.                                             ║
║    Partition: cscc-gpu-p                                                   ║
║                                                                            ║
║  • Debug lane (gpu-debug-qos) limits:                                      ║
║      Max Time=3 hours, MaxTRESPU=gres/gpu=4 (per-user), Max Jobs=2         ║
║      Priority: HIGH (debug jobs will recieve allocation faster)            ║
║                                                                            ║
║  • Interactive runs (srun/salloc):                                         ║
║      Forced into the debug lane: --qos=gpu-debug-qos on -p cscc-gpu-p      ║
║                                                                            ║
║  • Main QoS policy (cscc-gpu-qos):                                         ║
║      Batch-only (sbatch). Submit proper scripts here; no interactive runs. ║
║      Need CLI on the main QoS? Start a batch job and use ‘sattach’.        ║
║                                                                            ║
║  • IMPORTANT — “sleep” jobs are BANNED on cscc-gpu-p:                      ║
║      sbatch jobs that only hold an allocation (e.g., sleep) without running║
║      real workload/scripts are strictly prohibited and actively enforced   ║
║      Attempts to skirt enforcement may lead to account suspension.         ║
║                                                                            ║
║  • “cscc-gpu-qos” remain for real work with large                          ║
║    GPU/time limits.                                                        ║
║                                                                            ║
║  Quick examples:                                                           ║
║    $ srun  -p cscc-gpu-p --qos=gpu-debug-qos --gres=gpu:1                  ║
║            --time=01:00:00 --pty bash                                      ║
║    $ sbatch -p cscc-gpu-p --qos=cscc-gpu-qos job.sh                        ║
║    $ sattach <jobid>.<stepid>                                              ║
║                                                                            ║
║  → Questions? hpc.admins@mbzuai.ac.ae                                      ║
║  → More info: check our wiki at hpc.mbzuai.ac.ae/wiki/policies.html        ║
║                                                                            ║
║════════════════════════════════════════════════════════════════════════════║
║                                                                            ║
║                             Happy Computing!                               ║
║                                                                            ║
╚════════════════════════════════════════════════════════════════════════════╝
 0  ciai-login-1:~ $


